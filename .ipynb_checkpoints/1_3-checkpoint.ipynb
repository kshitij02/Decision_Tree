{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "#### Contrasting the effectiveness of Misclassification rate, Gini, Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as num\n",
    "import random\n",
    "import math\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_set(data,precentage):\n",
    "    indices=data.index.tolist()\n",
    "    test_size=(int)(precentage*len(data))\n",
    "    test_set_indices=random.sample(population=indices,k=test_size)\n",
    "    #print test_size\n",
    "    #print len(data)\n",
    "    test_set=data.loc[test_set_indices]\n",
    "    #print test_set_indices\n",
    "    train_set=data.drop(test_set_indices)\n",
    "    return train_set,test_set\n",
    "list_categorial_attribute=['Work_accident','promotion_last_5years','sales','salary']\n",
    "list_numeric_attribute=['satisfaction_level','last_evaluation','number_project','average_montly_hours','time_spend_company'\n",
    "]\n",
    "dic_avg_numeric={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading and Dividing data in Training and Vaildation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2248\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv(\"train.csv\")\n",
    "#data.head()\n",
    "# train_data ,test_data = train_test_set(data ,0.2)\n",
    "train_data=data.sample(frac=0.8,random_state=200)\n",
    "test_data=data.drop(train_data.index)\n",
    "test_data.head()\n",
    "# print len(test_data)\n",
    "# train_data.head()\n",
    "#test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is_pure fuction \n",
    "Tests wheather only one type data is left or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def is_pure(data):\n",
    "    current=data['left']\n",
    "    if len(set(current))==1:\n",
    "        return True \n",
    "    else: \n",
    "        return False\n",
    "is_pure(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calc _entropy function \n",
    "Calculates the entropy of given Data when type calc == 0 , gini of given Data when type calc == 1 & missclassification of given Data when type calc == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36703495788795115"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_entropy(data,type_calc):\n",
    "    current=data['left'].tolist()\n",
    "    if is_pure(data):\n",
    "        return 0\n",
    "    else:\n",
    "        num_one=current.count(1)\n",
    "        p=num_one/float(len(data.index))\n",
    "        if type_calc==0:\n",
    "            entropy= ((p*math.log(p,2)) + ((1-p)*math.log(1-p,2)))*-1\n",
    "            return entropy\n",
    "        elif type_calc==1:\n",
    "            gini_index=2*p*(1-p)\n",
    "            return gini_index\n",
    "        elif type_calc==2:\n",
    "            missclasification=min(p,1-p)\n",
    "            return missclasification\n",
    "calc_entropy(train_data,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calc_information function\n",
    "calculates the information in given data with respect each and every atttributes provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36442244659534634"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_information(data,attribute,type_calc):\n",
    "    current=set(data[attribute])\n",
    "    information=0\n",
    "    for cur in current:\n",
    "        current_data=data[data[attribute]==cur]\n",
    "        current_entropy=calc_entropy(current_data,type_calc)*(len(current_data.index)/float(len(data.index)))\n",
    "        information=information+current_entropy\n",
    "    return information\n",
    "calc_information(train_data,'sales',1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calc_information_gain function\n",
    " Calculates Information Gain of attribute on the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0026125112926048133"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_inforamtion_gain(data,attribute,type_calc):\n",
    "    return calc_entropy(data,type_calc)-calc_information(data,attribute,type_calc)\n",
    "calc_inforamtion_gain(train_data,'sales',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### best_information_gain function \n",
    "Find best information gain attributes of from all attributes of present in the list_categorial_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'salary'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_information_gain(data,list_categorial_attribute,type_calc):\n",
    "    dict_categorial_attribute={}\n",
    "    flag=0\n",
    "    min_v=0\n",
    "    min_at=''\n",
    "    for attr in list_categorial_attribute:\n",
    "        if flag==0:\n",
    "            min_v=calc_inforamtion_gain(data,attr,type_calc)\n",
    "            min_at=attr\n",
    "            flag=1\n",
    "        elif min_v<calc_inforamtion_gain(data,attr,type_calc):\n",
    "            min_v=min_v=calc_inforamtion_gain(data,attr,type_calc)\n",
    "            min_at=attr\n",
    "    return min_at\n",
    "best_information_gain(train_data,list_categorial_attribute,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### covert_numeric_to_categorical function\n",
    "Fucntion converts the numeric data to categorical data by assigning labels to the in place of numeric values of the attributes.\n",
    "First data is sorted according to the numeric value attribute and the all unique numeric values present in the numerical value attribute is obtained then for all the unique value we find out what is maximum number of time occuring in 'left' attribute of data and that value is assign to that unique value.And then data is accessed is sequential and where the value assigned to numeric value changes the average of numerical value is taken and value less that value is given one label and value greater than that avgerage falls under other label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numeric_to_categorical(data,attribute):\n",
    "    data.sort_values(by=[attribute],inplace=True)\n",
    "#     print data\n",
    "    dic={}\n",
    "    li=data[attribute].unique().tolist()\n",
    "    for l in li :\n",
    "        temp=data[data[attribute]==l]\n",
    "        dic[l]=num.unique(temp['left'])[num.argmax(num.unique(temp['left'], return_counts = True)[1])]\n",
    "#     print dic                                               \n",
    "    flag=0\n",
    "    avg_list=[]\n",
    "    count=0\n",
    "    for i in data.index :\n",
    "        if flag==0:\n",
    "            prev_elem=data.loc[i,attribute]\n",
    "            prev_left=dic[prev_elem]\n",
    "            flag=1\n",
    "        elif prev_left!=dic[data.loc[i,attribute]]:\n",
    "            avg_list.append((prev_elem+data.loc[i,attribute])/2.0)\n",
    "            prev_elem=data.loc[i,attribute]\n",
    "            prev_left=dic[prev_elem]\n",
    "            count=count+1\n",
    "#             print count\n",
    "        else :\n",
    "            prev_elem=data.loc[i,attribute]\n",
    "            prev_left=dic[prev_elem]\n",
    "#     print avg_list\n",
    "    dic_avg_numeric[attribute]=avg_list\n",
    "    label=0\n",
    "    list_label=[]\n",
    "    index_list=[]\n",
    "    f=0\n",
    "    for i in data.index :\n",
    "        label=0\n",
    "        index_list.append(i)\n",
    "        f=0\n",
    "        for j in avg_list:\n",
    "            if data.loc[i,attribute]<=j:\n",
    "                list_label.append(label)\n",
    "                f=1\n",
    "                break\n",
    "            else :\n",
    "                label=label+1\n",
    "        if f==0:\n",
    "            list_label.append(label)\n",
    "#     print pd.Series(list_label)\n",
    "    \n",
    "#     print len(data)\n",
    "#     print data.head()\n",
    "    \n",
    "    num_attribute=attribute+'_numeric'\n",
    "    data[num_attribute]=pd.Series(list_label,index=index_list)\n",
    "    \n",
    "#     print data.head()\n",
    "def label_prediction(data,attribute):\n",
    "    avg_list=dic_avg_numeric[attribute]\n",
    "    label=0\n",
    "    list_label=[]\n",
    "    index_list=[]\n",
    "    f=0\n",
    "    for i in data.index :\n",
    "        label=0\n",
    "        index_list.append(i)\n",
    "        f=0\n",
    "        for j in avg_list:\n",
    "            if data.loc[i,attribute]<=j:\n",
    "                list_label.append(label)\n",
    "                f=1\n",
    "                break\n",
    "            else :\n",
    "                label=label+1\n",
    "        if f==0:\n",
    "            list_label.append(label)\n",
    "    num_attribute=attribute+'_numeric'\n",
    "    data[num_attribute]=pd.Series(list_label,index=index_list)\n",
    "\n",
    "\n",
    "\n",
    "for attr in list_numeric_attribute:\n",
    "     convert_numeric_to_categorical(train_data,attr)\n",
    "# convert_numeric_to_categorical(train_data,'satisfaction_level')\n",
    "# print train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build_tree \n",
    "build_tree fuctions builds tree on the currrently present data in the form of dictionary of dictionary,while recurrsively calling itself again and agian and assigning attribute with best_gain_attribute as root of present tree untill one of following conditions is meet-\n",
    "1. Data size become zero \n",
    "2. No Attributes is present in attribute_list \n",
    "3. All data left column value is same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(table, prev_table, attribute_list,type_calc,tree = None):\n",
    "    if len(set(table['left'])) <= 1:\n",
    "        return {'leaf' : table['left'].tolist()[0]}\n",
    "    elif len(table) == 0:\n",
    "        return {'leaf': num.unique(prev_table['left'])[num.argmax(num.unique(prev_table['left'], return_counts = True)[1])]}\n",
    "    elif len(attribute_list) == 0:\n",
    "        return {'leaf': num.unique(prev_table['left'])[num.argmax(num.unique(prev_table['left'], return_counts = True)[1])]}\n",
    "    node = best_information_gain(table, attribute_list,type_calc)\n",
    "    attribute_list.remove(node)\n",
    "    if tree is None:\n",
    "        tree = {}\n",
    "        tree[node] = {}\n",
    "    for v in table[node].unique():\n",
    "        mod_table = table.where(table[node] == v).dropna()\n",
    "        tree[node][v] = build_tree(mod_table, table, attribute_list[:],type_calc)\n",
    "    return tree\n",
    "\n",
    "list_final=list_categorial_attribute\n",
    "for i in list_numeric_attribute:\n",
    "    list_final.append(i+'_numeric')\n",
    "    \n",
    "tree_entropy = build_tree(train_data, train_data, list_final[:],0)\n",
    "tree_gini = build_tree(train_data, train_data, list_final[:],1)\n",
    "tree_missclassification = build_tree(train_data, train_data, list_final[:],2)\n",
    "# pprint.pprint(tree_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict_helper and predict fucntion :\n",
    "Function predict_helper is called on whole validation_data and then predict fucntion is called on each row one by one and  tree build throung train data is used to predict the result of the given row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_helper(test_data , li , tree):\n",
    "#     print(test_data.index)\n",
    "    for i in test_data.index:\n",
    "#         print test_data.loc[i]\n",
    "        predict(test_data.loc[i],li,tree)\n",
    "    return li\n",
    "def predict(row,li,tree):\n",
    "    try:\n",
    "        if tree.keys()[0]=='leaf':\n",
    "            li.append(tree['leaf'])\n",
    "        else :\n",
    "            t=tree.keys()[0]\n",
    "    #         print t\n",
    "            value=row[t]\n",
    "    #         print tree\n",
    "    #         print row[t]\n",
    "    #          print tree[row[t]]\n",
    "            predict(row,li,tree[t][value])\n",
    "    except:\n",
    "        li.append(0.0) #default value\n",
    "# print len(pridected_value)\n",
    "# print pridected_value.count(1)\n",
    "# print pridected_value.count(0)\n",
    "for attr in list_numeric_attribute:\n",
    "     label_prediction(test_data,attr)\n",
    "# convert_numeric_to_categorical(test_data,'satisfaction_level')\n",
    "pridected_value_entropy=[]\n",
    "pridected_value_entropy= predict_helper(test_data,pridected_value_entropy,tree_entropy)\n",
    "pridected_value_gini=[]\n",
    "pridected_value_gini= predict_helper(test_data,pridected_value_gini,tree_gini)\n",
    "pridected_value_missclassification=[]\n",
    "pridected_value_missclassification= predict_helper(test_data,pridected_value_missclassification,tree_missclassification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calc_prefomance function\n",
    "Calcalutes the preformance of the predict values from the tree produce through the build_tree with respect to it's actual value\n",
    "And give Accuracy,Precision ,Recall & F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value = test_data['left'].tolist()\n",
    "# print len(target_value)\n",
    "def calc_preformance(target_value,pridected_value):\n",
    "    t_p=0\n",
    "    f_p=0\n",
    "    t_n=0\n",
    "    f_n=0\n",
    "    for i in range(len(target_value)):\n",
    "        if target_value[i]==0 and target_value[i]==pridected_value[i]:\n",
    "            t_n=t_n+1\n",
    "        elif target_value[i]==1 and target_value[i]==pridected_value[i]:\n",
    "            t_p=t_p+1\n",
    "        elif pridected_value[i]==1 and target_value[i]==0:\n",
    "            f_p=f_p+1\n",
    "        elif pridected_value[i]==0 and target_value[i]==1:\n",
    "            f_n=f_n+1\n",
    "    accuracy=(t_n+t_p)/float(t_n+t_p+f_p+f_n)\n",
    "    \n",
    "    precison=(t_p)/float(t_p+f_p)\n",
    "    recall=(t_p)/float(t_p+f_n)\n",
    "    a=1/precison\n",
    "    b=1/recall\n",
    "    f1_score=2/(a+b)\n",
    "#     print \"ture positive\",t_p\n",
    "#     print \"false positive\",f_p\n",
    "#     print \"false negative\",f_n\n",
    "#     print \"ture negative\",t_n\n",
    "    \n",
    "    print \"Accuracy \",accuracy\n",
    "    print \"Precision \",precison\n",
    "    print \"Recall \",recall\n",
    "    print \"F1 Score\",f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Precision Recall F1 Score with Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print (\"Entropy\")\n",
    "calc_preformance(target_value,pridected_value_entropy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Precision Recall F1 Score with Gini Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini Index\n",
      "Accuracy  0.958185053381\n",
      "Precision  0.899209486166\n",
      "Recall  0.913654618474\n",
      "F1 Score 0.906374501992\n"
     ]
    }
   ],
   "source": [
    "print (\"Gini Index\")\n",
    "calc_preformance(target_value,pridected_value_gini)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Precision Recall F1 Score with Missclassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missclassification Rate\n",
      "Accuracy  0.962633451957\n",
      "Precision  0.914\n",
      "Recall  0.917670682731\n",
      "F1 Score 0.915831663327\n"
     ]
    }
   ],
   "source": [
    "print (\"Missclassification Rate\")\n",
    "calc_preformance(target_value,pridected_value_missclassification)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
